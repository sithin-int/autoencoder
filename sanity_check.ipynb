{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b991d9a2-84e8-4f21-8c11-81698c022746",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import time as time\n",
    "import copy as copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70626d02-36b6-4c4f-bcf3-a2799741adac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "XL_PATH = r\"radiomicsFeatures.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8021174f-0b86-42f4-9f0d-39d70d443e51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feats_df = pd.read_csv(XL_PATH)\n",
    "\n",
    "feats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f49ee3-2b5e-4c85-ad0f-13ecdceab6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc17915-8cc7-416e-a38e-d6bcf5163e8c",
   "metadata": {},
   "source": [
    "### Autoencoder Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa9c52f-ad80-471a-9b36-2aa85680be5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f23cb4e-551e-4421-a656-80f5e46c01b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.X[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab2dc7-3caf-4f61-8101-31d5ec9d729e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MixupDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        i, j = np.random.randint(len(self.X), size=2)\n",
    "        \n",
    "        p = np.random.uniform()\n",
    "        \n",
    "        x = p * self.X[i] + (1-p) * self.X[j]\n",
    "        \n",
    "        return x, x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45849542-1d55-4972-ae17-3f3cff7860e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19d579-a051-4b4e-9321-6d2d4f94ca12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FC_Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_feats, hidden_layers, activation_fn = nn.LeakyReLU()):\n",
    "        \n",
    "        super(FC_Block, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        for out_feats in hidden_layers:\n",
    "            layers += [nn.Linear(in_feats, out_feats), activation_fn]\n",
    "            in_feats = out_feats\n",
    "            \n",
    "        self.block = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.block(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7c8add-56ee-4b05-96bd-c38e116121a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, encoder_layers=[100,50,25], latent_dim=5, activation_fn = nn.LeakyReLU()):\n",
    "        \n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder_block = FC_Block(input_dim, encoder_layers, activation_fn)\n",
    "        \n",
    "        self.embedding_layer = nn.Sequential(*[nn.Linear(encoder_layers[-1], latent_dim), activation_fn])\n",
    "        \n",
    "        decoder_layers = list(reversed(encoder_layers))\n",
    "        self.decoder_block = FC_Block(latent_dim, decoder_layers, activation_fn)\n",
    "        self.scores = nn.Linear(decoder_layers[-1], input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.encoder_block(x)\n",
    "        h = x = self.embedding_layer(x)\n",
    "        x = self.decoder_block(x)\n",
    "        x = self.scores(x)\n",
    "        \n",
    "        return x, h\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99f6c92-60c4-4e92-89c6-a5466c87e5dc",
   "metadata": {},
   "source": [
    "##### Training/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e811300-e230-4256-9b92-9ac7895e5673",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self, net):\n",
    "        self.net = net\n",
    "        \n",
    "    def compile(self, lr, h_lambda, loss_fn, cuda_device_id=0):\n",
    "        \n",
    "        self.h_lambda = h_lambda\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr)\n",
    "        self.loss_fn = loss_fn \n",
    "        self.device = torch.device(f\"cuda:{cuda_device_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.net.to(self.device)\n",
    "        \n",
    "    def prepare_minibatch(self, mini_batch):\n",
    "        \n",
    "        inputs, targets = mini_batch\n",
    "        \n",
    "        return inputs.float().to(self.device), targets.float().to(self.device)\n",
    "        \n",
    "    def fit(self, dls, num_epochs, verbose=True):\n",
    "        \n",
    "        since = time.time()\n",
    "        \n",
    "        hist = {'train':{'loss':[]}, 'val':{'loss':[]}}\n",
    "        \n",
    "        best_loss = np.inf\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            if verbose:\n",
    "                \n",
    "                print('Epoch {}/{}'.format(epoch,num_epochs-1))\n",
    "                print('-'*10)\n",
    "                \n",
    "            for phase in [\"train\", \"val\"]:\n",
    "                \n",
    "                if phase==\"train\":\n",
    "                    self.net.train()\n",
    "                else:\n",
    "                    self.net.eval()\n",
    "                    \n",
    "                running_loss = 0.0\n",
    "                \n",
    "                for mini_batch in dls[phase]:\n",
    "                    \n",
    "                    self.optimizer.zero_grad()\n",
    "                    \n",
    "                    inputs, targets = self.prepare_minibatch(mini_batch)\n",
    "                    \n",
    "                    with torch.set_grad_enabled(phase==\"train\"):\n",
    "                        \n",
    "                        recon_inputs, h = self.net(inputs)\n",
    "                        \n",
    "                        loss = self.loss_fn(recon_inputs, targets) + self.h_lambda * h.flatten().abs().sum()\n",
    "                        \n",
    "                        if phase==\"train\":\n",
    "                            \n",
    "                            loss.backward()\n",
    "                            self.optimizer.step()\n",
    "                            \n",
    "                        running_loss += loss.item()\n",
    "                            \n",
    "                epoch_loss = running_loss/len(dls[phase])\n",
    "                hist[phase][\"loss\"].append(epoch_loss)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"{} Loss :{:.4f}\".format(phase,epoch_loss))\n",
    "                    \n",
    "                if phase == \"val\":\n",
    "                    \n",
    "                    if epoch_loss<best_loss:\n",
    "                        best_loss = epoch_loss\n",
    "                        best_model_wts = copy.deepcopy(self.net.state_dict())\n",
    "                        if verbose:\n",
    "                            print(f\"Checkpoing made at {epoch}\")\n",
    "                        \n",
    "            if verbose:\n",
    "                print()\n",
    "                \n",
    "            \n",
    "        time_elapsed = time.time() - since\n",
    "        \n",
    "        \n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        print('Best val Loss: {:4f}'.format(best_loss)) \n",
    "\n",
    "        \n",
    "        self.net.load_state_dict(best_model_wts)\n",
    "        \n",
    "        return self.net.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1d156e-b218-45cd-8065-2abe7024c8f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def norm_anomaly_split(X, y):\n",
    "    \n",
    "    normal_indeces = np.argwhere(y==0).ravel()\n",
    "    anomaly_indeces = np.argwhere(y==1).ravel()\n",
    "    \n",
    "    X_norm = X[normal_indeces]\n",
    "    X_anomaly = X[anomaly_indeces]\n",
    "\n",
    "    return X_norm, X_anomaly\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31220cc-2f84-4ed8-b25e-fbe569c1b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_using_tsne(X, y, n_components=2):\n",
    "    \n",
    "    X_transformed = TSNE(n_components = n_components, random_state=0).fit_transform(X)\n",
    "    \n",
    "    plt.scatter(*zip(*X_transformed[y==1]), marker='o', color='r', s=10, label='Anomalous')\n",
    "    plt.scatter(*zip(*X_transformed[y==0]), marker='o', color='g', s=10, label='Normal')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf57083-a415-4983-8de4-15f4ba2401c3",
   "metadata": {},
   "source": [
    "### Sanity Check - Autoencoder Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94132c8d-7e37-45a9-95ba-5c0e44da0005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/mvaldenegro/UncertaintyML-course-ESSAI-labs\n",
    "# https://github.com/mvaldenegro/UncertaintyML-course-ESSAI-labs/blob/main/02_eval_uncertainty_calibration.ipynb\n",
    "# https://atcold.github.io/NYU-DLSP20/en/week01/01-3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c8754b-fe01-45f6-9d48-0543a05fb69f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feats = [column for column in feats_df.columns if column not in [\"id\",\"label\"]]\n",
    "print(len(feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104520ac-316a-4fa0-a4d0-935012e5b7a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = feats_df[feats].to_numpy()\n",
    "y = feats_df[\"label\"].to_numpy()\n",
    "\n",
    "# X = scaler.fit_transform(X)\n",
    "\n",
    "X_norm, X_anomaly = norm_anomaly_split(X, y)\n",
    "\n",
    "X_norm = scaler.fit_transform(X_norm) #this works better than the alternative where you standardize the whole X\n",
    "X_anomaly = scaler.transform(X_anomaly)\n",
    "\n",
    "X_norm = torch.from_numpy(X_norm).float()\n",
    "X_anomaly = torch.from_numpy(X_anomaly).float()\n",
    "\n",
    "_, input_dim = X.shape\n",
    "\n",
    "torch.manual_seed(0)\n",
    "idx = torch.randperm(len(X_norm))\n",
    "\n",
    "X_train = X_norm[idx[:-len(X_anomaly)]]\n",
    "\n",
    "X_test_norm = X_norm[idx[-len(X_anomaly):]]\n",
    "X_test_anomaly = X_anomaly\n",
    "\n",
    "# X_test = torch.concat([X_test_norm, X_test_anomaly])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff1abe0-d133-45f7-b759-57686d82933d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize_using_tsne(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2952458-50f7-4a33-8cad-d4992133b259",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10_000\n",
    "batch_size = 32\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "latent_dim = 5\n",
    "\n",
    "activation_fn = nn.LeakyReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dec0d7a-4217-4d22-a9a3-48f1b20df0f6",
   "metadata": {},
   "source": [
    "##### 1. Single Standard Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed5ec7d-01d9-4529-baad-b68beb298481",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = MixupDataset(X_train)\n",
    "val_ds = Dataset(X_train)\n",
    "dls = {\"train\":torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True),\"val\":torch.utils.data.DataLoader(val_ds, batch_size=batch_size)}\n",
    "\n",
    "h_lambda = 0.0 #disabling l1 sparsity constraint\n",
    "encoder_layers = [50, 25, 10] #under-complete hidden layers\n",
    "standard_ae = Autoencoder(input_dim, encoder_layers=encoder_layers, latent_dim=latent_dim, activation_fn = activation_fn)\n",
    "\n",
    "model = Model(standard_ae)\n",
    "model.compile(lr, h_lambda, loss_fn)\n",
    "_ = model.fit(dls, num_epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefdfcf3-0e0a-41c9-b0a3-a4d24a2bbd29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recon_X_test_norm, h = model.net(X_test_norm)\n",
    "recon_X_test_anomaly, h = model.net(X_test_anomaly)\n",
    "\n",
    "normal_mse = nn.MSELoss(reduction=\"none\")(recon_X_test_norm, X_test_norm).mean(axis=0).detach().numpy()\n",
    "anomaly_mse = nn.MSELoss(reduction=\"none\")(recon_X_test_anomaly, X_test_anomaly).mean(axis=0).detach().numpy()\n",
    "\n",
    "delta = anomaly_mse - normal_mse\n",
    "rank = len(delta) - (delta.argsort().argsort() + 1) + 1\n",
    "\n",
    "print(normal_mse.mean(), anomaly_mse.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01aaf1c-28c8-4a22-af59-53e06f39a5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(normal_mse)\n",
    "plt.hist(anomaly_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5032b54-4014-4620-95d3-d597f8e63667",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 2. Single Sparse Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797cd322-0764-450e-ac28-b2f4f82013bb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = Dataset(X_train)\n",
    "val_ds = Dataset(X_train)\n",
    "dls = {\"train\":torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True),\"val\":torch.utils.data.DataLoader(val_ds, batch_size=batch_size)}\n",
    "\n",
    "h_lambda = 1e-2 #with l1 regularization\n",
    "encoder_layers = [50, 25, 10] #under-complete hidden layers\n",
    "sparse_ae = Autoencoder(input_dim, encoder_layers=encoder_layers, latent_dim=latent_dim, activation_fn = activation_fn)\n",
    "\n",
    "model = Model(sparse_ae)\n",
    "model.compile(lr, h_lambda, loss_fn)\n",
    "_ = model.fit(dls, num_epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1343c93-d976-4995-9b04-50a13f9a9836",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recon_X_test_norm, h = model.net(X_test_norm)\n",
    "recon_X_test_anomaly, h = model.net(X_test_anomaly)\n",
    "\n",
    "normal_mse = nn.MSELoss(reduction=\"none\")(recon_X_test_norm, X_test_norm).mean(axis=0).detach().numpy()\n",
    "anomaly_mse = nn.MSELoss(reduction=\"none\")(recon_X_test_anomaly, X_test_anomaly).mean(axis=0).detach().numpy()\n",
    "\n",
    "delta = anomaly_mse - normal_mse\n",
    "rank = len(delta) - (delta.argsort().argsort() + 1) + 1\n",
    "\n",
    "print(normal_mse.mean(), anomaly_mse.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8fc5a5-713a-40fa-93d8-1ce42f9f2167",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(normal_mse)\n",
    "plt.hist(anomaly_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83456ce3-caa8-48b3-a9d6-b5fc96b0ac84",
   "metadata": {},
   "source": [
    "### Sanity Check - Stability Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd74e9a-6801-4f89-85b1-3c1f25bcbf62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf2fbea-d388-474f-a61c-ac779e15d395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7081757a-6d50-4c42-b805-b240d37d8ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dd58c2-e255-4fdc-8365-0f9d10e9d7d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0408cb7a-d3f7-4975-99c4-dd7feaf80b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe22dbc8-d1a8-4303-a600-46e255343301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4307533d-73ff-4bd3-be77-34065972898d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64a6ff3-d2f5-492d-9fc3-cbdafc280828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6256aa56-9bf4-4f60-954a-f7017f400a58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f3a16a-69c2-43d2-beb5-c6b46a545d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd265dff-d261-4566-a35b-9f0a0968eca1",
   "metadata": {},
   "source": [
    "##### 3. Ensemble Sparse Autoencoders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f89af25-66b6-4032-8fd6-66b92f0fdc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_count = 100\n",
    "\n",
    "num_epochs = 10_000\n",
    "batch_size = 32\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "latent_dim = 5\n",
    "\n",
    "activation_fn = nn.LeakyReLU()\n",
    "\n",
    "h_lambda = 1e-2 #with l1 regularization\n",
    "encoder_layers = [50, 25, 10] #under-complete hidden layers\n",
    "sparse_ae = Autoencoder(input_dim, encoder_layers=encoder_layers, latent_dim=latent_dim, activation_fn = activation_fn)\n",
    "\n",
    "results_df = {**{\"ae_id\":[], \"mse_mean\":[]}, **{\"mse_\"+f:[] for f in feats}, **{\"label\":[]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda1b24-6613-4fe0-b18f-31ad8d29112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ae_id in range(ensemble_count):\n",
    "    \n",
    "    print(\"*\"*50)\n",
    "    print(f\"Autoencoder ID: {ae_id+1}\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = feats_df[feats].to_numpy()\n",
    "    y = feats_df[\"label\"].to_numpy()\n",
    "\n",
    "    # X = scaler.fit_transform(X)\n",
    "\n",
    "    X_norm, X_anomaly = norm_anomaly_split(X, y)\n",
    "\n",
    "    X_norm = scaler.fit_transform(X_norm) #this works better than the alternative where you standardize the whole X\n",
    "    X_anomaly = scaler.transform(X_anomaly)\n",
    "\n",
    "    X_norm = torch.from_numpy(X_norm).float()\n",
    "    X_anomaly = torch.from_numpy(X_anomaly).float()\n",
    "\n",
    "    _, input_dim = X.shape\n",
    "\n",
    "    # torch.manual_seed(0) #this is where each ensemble run essentially differ, \n",
    "    idx = torch.randperm(len(X_norm))\n",
    "\n",
    "    X_train = X_norm[idx[:-len(X_anomaly)]]\n",
    "\n",
    "    X_test_norm = X_norm[idx[-len(X_anomaly):]]\n",
    "    X_test_anomaly = X_anomaly\n",
    "\n",
    "    \n",
    "    train_ds = Dataset(X_train)\n",
    "    val_ds = Dataset(X_train)\n",
    "    dls = {\"train\":torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True),\"val\":torch.utils.data.DataLoader(val_ds, batch_size=batch_size)}\n",
    "\n",
    "    model = Model(sparse_ae)\n",
    "    model.compile(lr, h_lambda, loss_fn)\n",
    "    _ = model.fit(dls, num_epochs, verbose=False)\n",
    "    \n",
    "    recon_X_test_norm, h = model.net(X_test_norm)\n",
    "    recon_X_test_anomaly, h = model.net(X_test_anomaly)\n",
    "\n",
    "    normal_mse = nn.MSELoss(reduction=\"none\")(recon_X_test_norm, X_test_norm).mean(axis=0).detach().numpy()\n",
    "    anomaly_mse = nn.MSELoss(reduction=\"none\")(recon_X_test_anomaly, X_test_anomaly).mean(axis=0).detach().numpy()\n",
    "    \n",
    "    \n",
    "    results_df[\"ae_id\"].append(ae_id+1)\n",
    "    results_df[\"mse_mean\"].append(normal_mse.mean())\n",
    "    for f, f_normal_mse in zip(feats, normal_mse):\n",
    "        results_df[\"mse_\"+f].append(f_normal_mse)\n",
    "    results_df[\"label\"].append(0)\n",
    "        \n",
    "    results_df[\"ae_id\"].append(ae_id+1)\n",
    "    results_df[\"mse_mean\"].append(anomaly_mse.mean())\n",
    "    for f, f_anomaly_mse in zip(feats, anomaly_mse):\n",
    "        results_df[\"mse_\"+f].append(f_anomaly_mse)\n",
    "    results_df[\"label\"].append(1)\n",
    "        \n",
    "    print(\"normal_mse=\", normal_mse.mean(), \"anomaly_mse=\", anomaly_mse.mean())\n",
    "    \n",
    "results_df = pd.DataFrame(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ab4aa-3288-46c2-a7e1-168c8e38edbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"results_df.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
